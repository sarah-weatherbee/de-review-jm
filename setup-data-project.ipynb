{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8088c83a-f802-43c6-830c-736c57e51585",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Make sure to follow the **[setup instructions here before starting](https://github.com/josephmachado/de_project?tab=readme-ov-file#option-1-github-codespaces-recommended)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2839c497-a703-47e8-b875-f1dca2dcbeb5",
   "metadata": {},
   "source": [
    "# Note: The recommended readings mentioned in this workshop(& more) will be covered in detail as part of my Data Engineering Hands-on Workshop, **[sign up here](https://astounding-architect-5764.ck.page/684e1f422f)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599d856-e7e3-4689-aab8-6f9c34468d10",
   "metadata": {},
   "source": [
    "# Live Workshop Link:\n",
    "\n",
    "[![Live workshop](https://img.youtube.com/vi/bfiOLwp1aWM/0.jpg)](https://www.youtube.com/live/bfiOLwp1aWM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d7cf4-f2dd-4004-b247-75907a5511d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "There are a lot of data projects available on the web (e.g., **[my list of data eng projects](https://www.startdataengineering.com/post/data-engineering-projects/)**). While these projects are great, starting from scratch to build your data project can be challenging. If you are \n",
    "\n",
    "> Wondering how to go from an idea to a production-ready data pipeline\n",
    "\n",
    "> Feeling overwhelmed by how all the parts of a data system fit together\n",
    "\n",
    "> Unsure that the pipelines you build are up to industry-standard\n",
    "\n",
    "If so, this post is for you! In it, we will go over how to build a data project step-by-step from scratch.\n",
    "\n",
    "By the end of this post, you will be able to quickly create data projects for any use case and see how the different parts of data systems work together. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603c6cf-c3e5-4103-bf08-9b079a1d325a",
   "metadata": {},
   "source": [
    "# Parts of data project\n",
    "\n",
    "Most data engineering tool falls into one of the parts shown below (as explained in this [post](https://www.startdataengineering.com/post/parts-of-dataengineering/))\n",
    "\n",
    "![Data tools](./assets/images/data-tools.png)\n",
    "\n",
    "In this post, we will review the parts of a data project and select tools to build a data pipeline. While we chose TPCH data for this project, anyone can choose any data set they find interesting and follow the below steps to quickly build their data pipeline.\n",
    "\n",
    "### **Recommended reading**: **[What are the key parts of data engineering](https://www.startdataengineering.com/post/parts-of-dataengineering/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad6b23-067c-47d5-b8de-01b8a21e887c",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "The first step before you start should be defining precise requirements. Please work with the end users to define them (or define them yourself for side projects).\n",
    "\n",
    "We will go over a few key requirements below. \n",
    "\n",
    "### **Recommended reading**: **[this post that goes over how to gather requirements for data projects in detail!](https://www.startdataengineering.com/post/n-questions-data-pipeline-req/)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1218fd-cae6-49e5-b1e4-2b8ac4eceb70",
   "metadata": {},
   "source": [
    "## Understand input datasets available\n",
    "\n",
    "Let's assume we are working with a car part seller database (tpch). The data is available in a duckdb database. See the data model below:\n",
    "\n",
    "![TPCH data model](./assets/images/tpch_erd.png)\n",
    "\n",
    "We can create fake input data using the [create_input_data.py](https://github.com/josephmachado/de_project/blob/main/setup/create_input_data.py) as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196b6e1d-684a-4f1f-b048-7f9421e9ad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up tpch and metadata db files\n",
      "Creating TPCH input data\n",
      "Creating metadata table\n"
     ]
    }
   ],
   "source": [
    "! python ./setup/create_input_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed8600-727f-4a94-8f13-74d2210657c4",
   "metadata": {},
   "source": [
    "TPCH data is well modeled and because of this its easy to work with. However this is not always the case, in most real life projects you'd need to \n",
    "\n",
    "1. Identify grain(aka what one row in a table corresponds to) of the input data. At times there may be tables with multiple grains.\n",
    "2. Identify the business process that generates the data. This will dictate how you can actually extract input datasets. For this you'll need to create **[conceptual & logical data models](https://www.thoughtspot.com/data-trends/data-modeling/conceptual-vs-logical-vs-physical-data-models)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760073a-874c-4e50-9840-b2e785f9fc15",
   "metadata": {},
   "source": [
    "**Exercise** what is the relationship between the `orders` table and `customer` table.\n",
    "\n",
    "a. 1 to many\n",
    "\n",
    "b. many to 1\n",
    "\n",
    "c. many to many\n",
    "\n",
    "d. 1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c88979-1fb7-43e3-b4b4-75da641be319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your choice (a, b, c, d) here\n",
    "answer = 'b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce2a1c-8e62-4397-b18d-a4821ba82b58",
   "metadata": {},
   "source": [
    "**Exercise**: what the relationship between the `customer` table and the `orders` table?\n",
    "\n",
    "a. 1 to many\n",
    "\n",
    "b. many to 1\n",
    "\n",
    "c. many to many\n",
    "\n",
    "d. 1 to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884f698e-cba0-4968-94d8-056d731d107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your choice (a, b, c, d) here\n",
    "answer = 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792878dc-081c-4d8c-9fcc-12e6b744891a",
   "metadata": {},
   "source": [
    "## Define what the output dataset will look like\n",
    "\n",
    "Let's assume that the `customer` team has asked us to create a dataset that they will use for outreach (think cold emails, calls, etc.). \n",
    "\n",
    "Upon discussion with the `customer` team, you discover that the output dataset requires the following columns:\n",
    "\n",
    "For each customer (i.e., one row per customer)\n",
    "\n",
    "1. **customer_key**: The unique identifier for the customer \n",
    "2. **customer_name**: The customer name\n",
    "3. **min_order_value**: The value of the order with the lowest value placed by this customer\n",
    "4. **max_order_value**: The value of the order with the highest value placed by this customer\n",
    "5. **avg_order_value**: The average value of all the orders placed by this customer  \n",
    "6. **avg_num_items_per_order**: The average number of items per order placed by this customer\n",
    "\n",
    "Let's write a simple query to see how we can get this (note that this process will take much longer with badly modeled input data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29118f12-3b06-41b3-952a-240fa7fa8408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────┬────────────────────┬─────────────────┬─────────────────┬────────────────────┬─────────────────────────┐\n",
       "│ customer_key │   customer_name    │ min_order_value │ max_order_value │  avg_order_value   │ avg_num_items_per_order │\n",
       "│    int64     │      varchar       │  decimal(15,2)  │  decimal(15,2)  │       double       │         double          │\n",
       "├──────────────┼────────────────────┼─────────────────┼─────────────────┼────────────────────┼─────────────────────────┤\n",
       "│          445 │ Customer#000000445 │        59180.35 │       280273.39 │ 144249.92470588235 │      4.0588235294117645 │\n",
       "│          557 │ Customer#000000557 │         4010.45 │       302534.45 │ 123190.81181818181 │       3.272727272727273 │\n",
       "│          611 │ Customer#000000611 │        37857.53 │       226132.40 │  95871.07363636364 │       3.272727272727273 │\n",
       "│          818 │ Customer#000000818 │        23233.65 │       326565.37 │ 176473.52384615384 │       4.230769230769231 │\n",
       "│          322 │ Customer#000000322 │        35831.73 │       273665.36 │ 137602.85857142857 │       4.142857142857143 │\n",
       "│          163 │ Customer#000000163 │        49760.53 │       328804.37 │ 133322.93857142856 │       3.642857142857143 │\n",
       "│          286 │ Customer#000000286 │         2767.39 │       301968.79 │ 141113.89157894737 │       4.052631578947368 │\n",
       "│           34 │ Customer#000000034 │        50473.48 │       308920.79 │ 164393.27958333332 │                   4.625 │\n",
       "│         1078 │ Customer#000001078 │        17384.33 │       340782.49 │ 120262.49966666667 │      3.3666666666666667 │\n",
       "│          826 │ Customer#000000826 │        39398.24 │       331740.14 │       160536.14375 │       4.333333333333333 │\n",
       "│           ·  │         ·          │            ·    │           ·     │             ·      │               ·         │\n",
       "│           ·  │         ·          │            ·    │           ·     │             ·      │               ·         │\n",
       "│           ·  │         ·          │            ·    │           ·     │             ·      │               ·         │\n",
       "│          341 │ Customer#000000341 │        36800.50 │       276702.99 │ 135387.83555555556 │       3.888888888888889 │\n",
       "│         1432 │ Customer#000001432 │        14121.10 │       268693.22 │  89107.60076923078 │      2.5384615384615383 │\n",
       "│          962 │ Customer#000000962 │         9136.24 │       243303.90 │ 103301.19692307692 │       3.230769230769231 │\n",
       "│          389 │ Customer#000000389 │        27250.86 │       207828.01 │         104641.156 │                     2.2 │\n",
       "│         1139 │ Customer#000001139 │       115629.77 │       228030.19 │          170359.49 │                    4.75 │\n",
       "│          896 │ Customer#000000896 │        95375.62 │       194365.23 │         151887.198 │                     4.8 │\n",
       "│          491 │ Customer#000000491 │         5532.94 │       362117.55 │ 155545.69363636363 │       4.090909090909091 │\n",
       "│         1274 │ Customer#000001274 │        14822.09 │       306798.37 │ 125342.42818181818 │      3.3636363636363638 │\n",
       "│          185 │ Customer#000000185 │        25352.65 │       144995.56 │           72334.53 │                     2.0 │\n",
       "│          373 │ Customer#000000373 │         3544.97 │       319477.22 │ 141204.63727272727 │      3.5454545454545454 │\n",
       "├──────────────┴────────────────────┴─────────────────┴─────────────────┴────────────────────┴─────────────────────────┤\n",
       "│ 1000 rows (20 shown)                                                                                       6 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple query to get the output dataset\n",
    "import duckdb\n",
    "con = duckdb.connect(\"tpch.db\")\n",
    "con.sql(\"\"\"\n",
    "WITH order_items AS (\n",
    "    SELECT\n",
    "        l_orderkey,\n",
    "        COUNT(*) AS item_count\n",
    "    FROM\n",
    "        lineitem\n",
    "    GROUP BY\n",
    "        l_orderkey\n",
    "),\n",
    "customer_orders AS (\n",
    "    SELECT\n",
    "        o.o_custkey,\n",
    "        o.o_orderkey,\n",
    "        o.o_totalprice,\n",
    "        oi.item_count\n",
    "    FROM\n",
    "        orders o\n",
    "    JOIN\n",
    "        order_items oi ON o.o_orderkey = oi.l_orderkey\n",
    ")\n",
    "SELECT\n",
    "    c.c_custkey AS customer_key,\n",
    "    c.c_name AS customer_name,\n",
    "    MIN(co.o_totalprice) AS min_order_value,\n",
    "    MAX(co.o_totalprice) AS max_order_value,\n",
    "    AVG(co.o_totalprice) AS avg_order_value,\n",
    "    AVG(co.item_count) AS avg_num_items_per_order\n",
    "FROM\n",
    "    customer c\n",
    "JOIN\n",
    "    customer_orders co ON c.c_custkey = co.o_custkey\n",
    "GROUP BY\n",
    "    c.c_custkey, c.c_name;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e66b874f-69b6-473c-be4c-ff0ab0e9bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()\n",
    "con.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8063abf-6654-4f8b-b038-b3efdf05dcb8",
   "metadata": {},
   "source": [
    "## Define SLAs so stakeholders know what to expect\n",
    "\n",
    "SLAs stand for service level agreement. SLAs define what end-users can expect from your service(data pipeline, in our case). While there are multiple ways to define SLAs, the common ones for data systems are:\n",
    "\n",
    "1. `Data freshness`\n",
    "2. `Data accuracy`\n",
    "\n",
    "Let's assume that our stakeholders require the data to be no older than 12 hours. This means that your pipeline should run completely at least once every 12 hours. If we assume that the pipeline runs in 2 hours, we need to ensure that it is run at least every 10 hours so that the data is not older than 12 hours at any given time.\n",
    "\n",
    "For data accuracy, we should define what accurate data is. Let's define accuracy in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f25f7b-b3e6-4bda-ba4b-d1d98aae2b34",
   "metadata": {},
   "source": [
    "## Define checks to ensure the output dataset is usable\n",
    "\n",
    "We need to ensure that the data we produce is good enough for end-users to use. Typically, the data team works with end users to identify the critical metrics to check. \n",
    "\n",
    "Let's assume we have the following checks to ensure that the output dataset is accurate:\n",
    "\n",
    "1. **customer_key**: Has to be unique and not null\n",
    "2. **avg_***: columns should not differ by more than 5% compared to prior runs (across all customers)\n",
    "\n",
    "### **Recommended reading**: **[Types of data quality checks](https://www.startdataengineering.com/post/types-of-dq-checks/)** & **[Implementing data quality checks with Great Expectations](https://www.startdataengineering.com/post/implement_data_quality_with_great_expectations/)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe33cbb-a2ff-49f2-bc3c-c5368da9bb02",
   "metadata": {},
   "source": [
    "**Exercise** What other (1 main) check can you think of for the output dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d595d0-fd16-4766-b712-907648a36579",
   "metadata": {},
   "source": [
    "# Identify what tool to use to process data\n",
    "\n",
    "We have a plethora of tools to process data, including Apache Spark, Snowflake, Python, Polars, and DuckDB. We will use Polars to process our data because it is small. The Polars library is easy to install and use.\n",
    "\n",
    "### **Recommended reading**: **[Choosing tools for your data project](https://www.startdataengineering.com/post/choose-tools-dp/#41-requirement-x-component-framework)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e82ab1-b56f-4a4a-a100-bace58dfb9c4",
   "metadata": {},
   "source": [
    "# Data flow architecture\n",
    "\n",
    "Most data teams have their version of the 3-hop architecture. For example, dbt has its own version (stage, intermediate, mart), and Spark has medallion (bronze, silver, gold) architecture.\n",
    "\n",
    "You may be wondering why we need this data flow architecture when we have **[the results easily with a simple query shown here](./setup-data-project.ipynb#Columns-and-metics-needed-in-the-dataset-produced)**. \n",
    "\n",
    "While this is a simple example, in most real-world projects you want to have a standard, cleaned and modelled dataset(bronze) that can be use to create specialized dataset for end-users(gold). See below for how our data will flow:\n",
    "\n",
    "![Data Flow](./assets/images/dep-arch.png)\n",
    "\n",
    "### **Recommended reading**: **[Multi-hop architecture](https://www.startdataengineering.com/post/de_best_practices/#31-use-standard-patterns-that-progressively-transform-your-data)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa70beaa-6dbc-412e-b468-96d72ca18622",
   "metadata": {},
   "source": [
    "## Bronze: Extract raw data and confine it to standard names and data types \n",
    "\n",
    "Since our dataset has data from customer, nation, region, order, and lineitem input datasets, we will bring those data into bronze tables. We will keep their names the same as the input datasets.\n",
    "\n",
    "Let's explore the input datasets and create our bronze datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08122bcc-e005-4139-8a36-b854ea30c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read customer, order, and lineitem dataset from duckdb into Polars dataframe\n",
    "import duckdb\n",
    "import polars as pl\n",
    "\n",
    "con = duckdb.connect(\"tpch.db\")\n",
    "customer_df = con.sql(\"select * from customer\").pl()\n",
    "orders_df = con.sql(\"select * from orders\").pl()\n",
    "lineitem_df = con.sql(\"select * from lineitem\").pl()\n",
    "nation_df = con.sql(\"select * from nation\").pl()\n",
    "region_df = con.sql(\"select * from region\").pl()\n",
    "\n",
    "con.close() #close DuckDB connection\n",
    "\n",
    "# remove c_ and then rename custkey to customer_key\n",
    "cleaned_customer_df = customer_df.rename(lambda col_name: col_name[2:]).rename({\"custkey\": \"customer_key\"})\n",
    "\n",
    "# remove the n_ and r_ from the nation and region table's column names\n",
    "cleaned_nation_df = nation_df.rename(lambda col_name: col_name[2:])\n",
    "cleaned_region_df = region_df.rename(lambda col_name: col_name[2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69049d33-13ce-4f68-a146-91011e449244",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Write code (similar to above)\n",
    "1. Remove the o_ and l_ from the order and lineitem table's column names\n",
    "2. We also rename customer key and order key to customer_key and order_key respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c21fc91-8d15-4db3-8692-fce9667541e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c521a-e814-44da-9fc0-6b663ee4762e",
   "metadata": {},
   "source": [
    "## Silver: Model data for analytics\n",
    "\n",
    "In the silver layer, the datasets are modeled using one of the popular styles (e.g., Kimball, Data Vault, etc.). We will use Kimball's dimensional model, as it is the most commonly used one and can account for many use cases.\n",
    "\n",
    "### Data modeling\n",
    "\n",
    "We will create the following datasets\n",
    "\n",
    "1. **dim_customer**: A customer level table with all the necessary attributes of a customer. We will join nation and region data to the `cleaned_customer_df` to get all the attributes associated with a customer.\n",
    "2. **fct_orders**: An order level fact(an event that happened) table. This will be the same as `cleaned_orders_df` since the `orders` table has all the necessary details about the order and how it associates with dimension tables like `customer_key`.\n",
    "3. **fct_lineitem**: A lineitem (items that are part of an order) fact table. This table will be the same as `cleaned_lineitem_df` since the `lineitem` table has all the lineitem level details and keys to associate with dimension tables like `partkey` and `suppkey`.\n",
    "\n",
    "### **Recommended reading**: **[Data warehouse overview](https://www.startdataengineering.com/post/what-is-a-data-warehouse/#3-what-is-a-data-warehouse)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "640ea92d-31e3-4878-90aa-24eafb2d193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create customer dimension by left-joining all the necessary data\n",
    "dim_customer = cleaned_customer_df\\\n",
    ".join(cleaned_nation_df, on=\"nationkey\", how=\"left\", suffix=\"_nation\")\\\n",
    ".join(cleaned_region_df, on=\"regionkey\", how=\"left\", suffix=\"_region\")\\\n",
    ".rename({\n",
    "    \"name_nation\": \"nation_name\",\n",
    "    \"name_region\": \"region_name\",\n",
    "    \"comment_nation\": \"nation_comment\",\n",
    "    \"comment_region\": \"region_comment\"\n",
    "})\n",
    "\n",
    "# Most fact tables are direct data from the app\n",
    "fct_orders = cleaned_orders_df\n",
    "fct_lineitem = cleaned_lineitem_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04941142-588b-443e-874c-ed9acf153276",
   "metadata": {},
   "source": [
    "## Gold: Create tables for end-users\n",
    "\n",
    "The gold layer contains datasets required for the end user. The user-required datasets are fact tables joined with dimension tables aggregated to the necessary grain. In real-world projects, multiple teams/users ask for datasets with differing grains from the same underlying fact and dimension tables. While you can join the necessary tables and aggregate them individually for each ask, it leads to repeated code and joins.\n",
    "\n",
    "To avoid this issue, companies typically do the following:\n",
    "\n",
    "1. **OBT**: This is a fact table with multiple dimension tables left joined with it.\n",
    "2. **pre-aggregated table**: The OBT table rolled up to the end user/team requested grain. The pre-aggregated dataset will be the dataset that the end user accesses. By providing the end user with the exact columns they need, we can ensure that all the metrics are in one place and issues due to incorrect metric calculations by end users are significantly reduced. These tables act as our end-users SOT (source of truth). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb2a65-8fe5-4b14-969f-03cbc9317477",
   "metadata": {},
   "source": [
    "### OBT: Join the fact table with all its dimensions\n",
    "\n",
    "In our example, we have two fact tables, `fct_orders` and `fct_lineitem`. Since we only have one dimension, `dim_customer,` we can join `fct_orders` and `dim_customer` to create `wide_orders`. For our use case, we can keep `fct_lineitem` as `wide_lineitem`.\n",
    "\n",
    "That said, we can easily see a case where we might need to join `parts` and `supplier` data with `fct_lineitem` to get `wide_lineitem`. But since our use case doesn't require this, we can skip it!\n",
    "\n",
    "Let's create our OBT tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e7bbb84-9c84-48df-a653-09d13a38cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wide_orders table\n",
    "wide_orders = fct_orders.join(dim_customer, on=\"customer_key\", how=\"left\")\n",
    "\n",
    "# For our use case, we don't need more information at a lineitem level\n",
    "wide_lineitem = fct_lineitem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d3618-ca24-4f07-862c-324d86e423d3",
   "metadata": {},
   "source": [
    "**Exercise**: Assume that you have to create a `wide_lineitem` table with all the dimensions at its respective grain. What other tables will you left join with `fct_lineitem` table?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d562fe8a-e306-4905-8d29-dca2903d057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here, all lower case, use tables names from the data model\n",
    "tables_to_left_join = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11e605-96ba-48c2-a1f8-0a8227cff3f3",
   "metadata": {},
   "source": [
    "### Pre-aggregated tables: Aggregate OBTs to stakeholder-specific grain\n",
    "\n",
    "According to our **[data requirements](./setup-data-project.ipynb#Columns-and-metics-needed-in-the-dataset-produced)**, we need data from customer, orders, and lineitem. Since we already have customer and order data in `wide_orders`, we can join that with `wide_lineitem` to get the necessary data.\n",
    "\n",
    "We can call the final dataset `customer_outreach_metrics` (read **[this article that discusses the importance of naming](https://docs.getdbt.com/blog/on-the-importance-of-naming)**).\n",
    "\n",
    "Let's create our final dataset in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ea6cd9c-7556-4b67-af6d-068808222f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create customer_outreach_metrics\n",
    "\n",
    "# get the number of lineitems per order\n",
    "order_lineitem_metrics = wide_lineitem.group_by(pl.col(\"order_key\")).agg(pl.col(\"linenumber\").count().alias(\"num_lineitems\"))\n",
    "# join the above df with wide_orders and group by customer key in wide orders to get avg, min, max order value & avg num items per order\n",
    "customer_outreach_metrics = wide_orders\\\n",
    ".join(order_lineitem_metrics, on=\"order_key\", how=\"left\")\\\n",
    ".group_by(\n",
    "    pl.col(\"customer_key\"), \n",
    "    pl.col(\"name\").alias(\"customer_name\"))\\\n",
    ".agg(\n",
    "    pl.min(\"totalprice\").alias(\"min_order_value\"),\n",
    "    pl.max(\"totalprice\").alias(\"max_order_value\"),\n",
    "    pl.mean(\"totalprice\").alias(\"avg_order_value\"),\n",
    "    pl.mean(\"num_lineitems\").alias(\"avg_num_items_per_order\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a649f4c9-9070-4a13-85bd-507f2951c5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>customer_key</th><th>customer_name</th><th>min_order_value</th><th>max_order_value</th><th>avg_order_value</th><th>avg_num_items_per_order</th></tr><tr><td>i64</td><td>str</td><td>decimal[15,2]</td><td>decimal[15,2]</td><td>decimal[15,2]</td><td>f64</td></tr></thead><tbody><tr><td>1478</td><td>&quot;Customer#000001478&quot;</td><td>45943.93</td><td>320623.44</td><td>null</td><td>4.6</td></tr><tr><td>416</td><td>&quot;Customer#000000416&quot;</td><td>53224.01</td><td>279221.38</td><td>null</td><td>4.714286</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌──────────────┬────────────────┬────────────────┬────────────────┬────────────────┬───────────────┐\n",
       "│ customer_key ┆ customer_name  ┆ min_order_valu ┆ max_order_valu ┆ avg_order_valu ┆ avg_num_items │\n",
       "│ ---          ┆ ---            ┆ e              ┆ e              ┆ e              ┆ _per_order    │\n",
       "│ i64          ┆ str            ┆ ---            ┆ ---            ┆ ---            ┆ ---           │\n",
       "│              ┆                ┆ decimal[15,2]  ┆ decimal[15,2]  ┆ decimal[15,2]  ┆ f64           │\n",
       "╞══════════════╪════════════════╪════════════════╪════════════════╪════════════════╪═══════════════╡\n",
       "│ 1478         ┆ Customer#00000 ┆ 45943.93       ┆ 320623.44      ┆ null           ┆ 4.6           │\n",
       "│              ┆ 1478           ┆                ┆                ┆                ┆               │\n",
       "│ 416          ┆ Customer#00000 ┆ 53224.01       ┆ 279221.38      ┆ null           ┆ 4.714286      │\n",
       "│              ┆ 0416           ┆                ┆                ┆                ┆               │\n",
       "└──────────────┴────────────────┴────────────────┴────────────────┴────────────────┴───────────────┘"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_outreach_metrics.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa41ba-26b5-43a1-8145-3cff8352239c",
   "metadata": {},
   "source": [
    "**Question**: Why is `avg_order_value` all `null`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2c1bebe-9999-444c-84e4-b1efcb4ae370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n",
    "answer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b7e0c-d854-48f4-afc6-e57c7e57684c",
   "metadata": {},
   "source": [
    "# Data quality and code testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8edcd6-58b6-4606-9eeb-896aaa82f4d7",
   "metadata": {},
   "source": [
    "## Data quality implementation\n",
    "\n",
    "As part of our requirements, we saw that the output dataset needs to have \n",
    "1. Unique and distinct `customer_key`\n",
    "2. Variance in `avg_*` columns between runs should not be more than 5% (across all customers)\n",
    "\n",
    "While the first test is a simple check, the second one requires that we use the data from previous runs and compare it with the current run's data or store the sum(avg_*) of each run. Let's store the run-level metrics in a run_metadata table (in sqlite3).\n",
    "\n",
    "Our pipelines should run data quality checks before making the data available to your end users. This ensures that you can catch any issues before they can cause damage.\n",
    "\n",
    "![WAP pattern](./assets/images/wap.png)\n",
    "\n",
    "### **Recommended reading**: **[Types of data quality checks](https://www.startdataengineering.com/post/types-of-dq-checks/)**, **[Implementing data quality checks with Great Expectations](https://www.startdataengineering.com/post/implement_data_quality_with_great_expectations/)**, & **[Write-Audit-Publish pattern](https://www.startdataengineering.com/post/de_best_practices/#32-ensure-data-is-valid-before-exposing-it-to-its-consumers-aka-data-quality-checks)**\n",
    "\n",
    "Let's see how we can implement DQ checks in a Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0f878b9-8826-41a9-a271-50f2a540ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# get current run's metrics\n",
    "curr_metrics = json.loads(\n",
    "    customer_outreach_metrics\\\n",
    "    .select(\n",
    "        pl.col(\"avg_num_items_per_order\").alias(\"sum_avg_num_items_per_order\"),\n",
    "        pl.col(\"avg_order_value\").alias(\"sum_avg_order_value\")\n",
    "    )\\\n",
    "    .sum()\\\n",
    "    .write_json())[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2095ed50-2afc-4860-bf8a-2e529ea8de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store run metadata in a table\n",
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('metadata.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert data into the run_metadata table\n",
    "cursor.execute('''\n",
    "    INSERT INTO run_metadata (run_id, metadata)\n",
    "    VALUES (?, ?)\n",
    "''', ('2024-09-15-10-00', json.dumps(curr_metrics)))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60034e3f-e014-4cf8-b938-2cf5d8564219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that another run of the data pipeline has been completed\n",
    "\n",
    "# Get the most recent data from the run_metadata table\n",
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('metadata.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch the most recent row based on run_id\n",
    "cursor.execute('''\n",
    "    SELECT * FROM run_metadata\n",
    "    ORDER BY run_id DESC\n",
    "    LIMIT 1\n",
    "''')\n",
    "\n",
    "# Get the result\n",
    "most_recent_row = cursor.fetchone()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f6a003c-bec7-42b6-88fb-7a58aa6c9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most recent metric\n",
    "prev_metric = json.loads(most_recent_row[1])\n",
    "\n",
    "# get current metric\n",
    "# This assumes the pipeline is rerun\n",
    "curr_metric = json.loads(\n",
    "    customer_outreach_metrics\\\n",
    "    .select(\n",
    "        pl.col(\"avg_num_items_per_order\").alias(\"sum_avg_num_items_per_order\"),\n",
    "        pl.col(\"avg_order_value\").cast(int).alias(\"sum_avg_order_value\")\n",
    "    )\\\n",
    "    .sum()\\\n",
    "    .write_json())[0]\n",
    "\n",
    "# Compare with current data for variance percentage\n",
    "def percentage_difference(val1, val2):\n",
    "    if val1 == 0 and val2 == 0:\n",
    "        return 0.0\n",
    "    elif val1 == 0 or val2 == 0:\n",
    "        return 100.0\n",
    "    return abs((val1 - val2) / ((val1 + val2) / 2)) * 100\n",
    "\n",
    "prev_metric['sum_avg_order_value'] = int(float(prev_metric['sum_avg_order_value']))\n",
    "\n",
    "comparison = {}\n",
    "for key in curr_metric:\n",
    "    if key in prev_metric:\n",
    "        comparison[key] = percentage_difference(curr_metric[key], prev_metric[key])\n",
    "\n",
    "if prev_metric is None:\n",
    "    print('No prev metric')\n",
    "    \n",
    "# code to check if variance < 5\n",
    "for k, v in comparison.items():\n",
    "    if v >= 5:\n",
    "        raise Exception(f\"Difference for {k} is greater than 5%: {v}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1d7790a-ee75-4c42-9e08-1bb90b6a0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert current run data into the run_metadata table\n",
    "# Store run metadata in a table\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Get current timestamp and format it\n",
    "current_timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('metadata.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "comparison_json = json.dumps(comparison)\n",
    "\n",
    "# Insert data into the run_metadata table\n",
    "cursor.execute('''\n",
    "    INSERT INTO run_metadata (run_id, metadata)\n",
    "    VALUES (?, ?)\n",
    "''', (current_timestamp, comparison_json))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c070b-325a-4012-b141-43078d22d843",
   "metadata": {},
   "source": [
    "**Exercise:** There is a deliberate mistake in the above cell, what is it?\n",
    "\n",
    "**Hint** Is the code actually doing what it is supposed to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af97ecfd-8d29-4ead-bd7b-c0cb080a4854",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944af0b7-1485-4f14-9662-4dba6a2a4d42",
   "metadata": {},
   "source": [
    "# Code organization\n",
    "\n",
    "Deciding how to organize your code can be overwhelming. Typically, companies use one of the following options to organize code:\n",
    "\n",
    "1. Based on multi-hop architecture. E.g. **[see this dbt folder structure](https://github.com/dbt-labs/jaffle_shop_duckdb/tree/duckdb/models)**\n",
    "2. Based on existing company standards.\n",
    "\n",
    "### Folder structure\n",
    "\n",
    "We can use the following folder structure for our use case(and most real-life projects).\n",
    "\n",
    "![Folder structure](./assets/images/folder.png)\n",
    "\n",
    "Each file under the elt folder will have the code necessary to generate that dataset. The above folder structure enables anyone new to the project to quickly understand where the code to create a certain dataset will be.\n",
    "\n",
    "Now compare this with dbt's recommended project structure:\n",
    "\n",
    "![dbt folder structure](./assets/images/dbtps.png)\n",
    "\n",
    "*Note* dbt recommends use of its `semantic layer` over pre-aggregated layer. The `semantic layer` involves aggregation with every query(approx) which can lead to skyrocketing costs.\n",
    "\n",
    "### Code modularity\n",
    "\n",
    "We have the code to create the necessary tables; now, we have to put them into functions that are easy to use and maintain. \n",
    "\n",
    "#### **Recommended reading**: **[How to write modular python code](https://www.startdataengineering.com/post/code-patterns/#1-functional-design)**\n",
    "\n",
    "We will define the function `create_dataset` for each table in the Python script for our use case. Having a common named function will enable\n",
    "\n",
    "1. Consistent naming. For example: `dim_customer.create_dataset`, `customer_outreach_metrics.create_dataset`\n",
    "2. Pull out code commonalities into a base class. Moving code into a common base class will be covered in a future post.\n",
    "\n",
    "Let's see what functions we would want to include in the `de_project/etl/gold/pre-aggregated/customer_outreach_metrics.py.`\n",
    "\n",
    "**Note** We have moved code that involves reading/writing to metadata into **[de_project/utils/metadata.py](https://github.com/josephmachado/de_project/blob/main/de_project/utils/metadata.py)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66f58b52-dfcd-417f-add7-fbece129206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_project/utils/metadata.py\n",
    "import json\n",
    "\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "def get_latest_run_metrics():\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(\"metadata.db\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch the most recent row based on run_id\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT * FROM run_metadata\n",
    "        ORDER BY run_id DESC\n",
    "        LIMIT 1\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Get the result\n",
    "    most_recent_row = cursor.fetchone()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    return (\n",
    "        json.loads(most_recent_row[1])\n",
    "        if most_recent_row and len(most_recent_row) > 0\n",
    "        else None\n",
    "    )\n",
    "\n",
    "\n",
    "def insert_run_metrics(curr_metrics):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(\"metadata.db\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    curr_metrics_json = json.dumps(curr_metrics)\n",
    "\n",
    "    current_timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "    # Insert data into the run_metadata table\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO run_metadata (run_id, metadata)\n",
    "        VALUES (?, ?)\n",
    "    \"\"\",\n",
    "        (current_timestamp, curr_metrics_json),\n",
    "    )\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a106863d-24ed-42fe-bece-0755c005e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_project/etl/gold/pre-aggregated/customer_outreach_metrics.py\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "def create_dataset(wide_lineitem, wide_orders):\n",
    "    order_lineitem_metrics = wide_lineitem.group_by(pl.col(\"order_key\")).agg(\n",
    "        pl.col(\"linenumber\").count().alias(\"num_lineitems\")\n",
    "    )\n",
    "    return (\n",
    "        wide_orders.join(order_lineitem_metrics, on=\"order_key\", how=\"left\")\n",
    "        .group_by(pl.col(\"customer_key\"), pl.col(\"name\").alias(\"customer_name\"))\n",
    "        .agg(\n",
    "            pl.min(\"totalprice\").alias(\"min_order_value\"),\n",
    "            pl.max(\"totalprice\").alias(\"max_order_value\"),\n",
    "            pl.mean(\"totalprice\").alias(\"avg_order_value\"),\n",
    "            pl.mean(\"num_lineitems\").alias(\"avg_num_items_per_order\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def percentage_difference(val1, val2):\n",
    "    if val1 == 0 and val2 == 0:\n",
    "        return 0.0\n",
    "    elif val1 == 0 or val2 == 0:\n",
    "        return 100.0\n",
    "    return abs((val1 - val2) / ((val1 + val2) / 2)) * 100\n",
    "\n",
    "\n",
    "def check_no_duplicates(customer_outreach_metrics_df):\n",
    "    # check uniqueness\n",
    "    if (\n",
    "        customer_outreach_metrics_df.filter(\n",
    "            customer_outreach_metrics_df.select(pl.col(\"customer_key\")).is_duplicated()\n",
    "        ).shape[0]\n",
    "        > 0\n",
    "    ):\n",
    "        raise Exception(\"Duplicate customer_keys\")\n",
    "\n",
    "\n",
    "def check_variance(customer_outreach_metrics_df, perc_threshold=5):\n",
    "    prev_metric = get_latest_run_metrics()\n",
    "    if prev_metric is None:\n",
    "        return\n",
    "    prev_metric['sum_avg_order_value'] = int(float(prev_metric['sum_avg_order_value']))\n",
    "    curr_metric = json.loads(\n",
    "        customer_outreach_metrics_df.select(\n",
    "            pl.col(\"avg_num_items_per_order\").alias(\"sum_avg_num_items_per_order\"),\n",
    "            pl.col(\"avg_order_value\").cast(int).alias(\"sum_avg_order_value\"),\n",
    "        )\n",
    "        .sum()\n",
    "        .write_json()\n",
    "    )[0]\n",
    "    comparison = {}\n",
    "    for key in curr_metric:\n",
    "        if key in prev_metric:\n",
    "            comparison[key] = percentage_difference(curr_metric[key], prev_metric[key])\n",
    "\n",
    "    for k, v in comparison.items():\n",
    "        if v >= perc_threshold:\n",
    "            raise Exception(f\"Difference for {k} is greater than 5%: {v}%\")\n",
    "\n",
    "\n",
    "def validate_dataset(customer_outreach_metrics_df):\n",
    "    # data quality checks\n",
    "    check_no_duplicates(customer_outreach_metrics_df)\n",
    "    check_variance(customer_outreach_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bde44-87c1-42a4-96ab-3fc88a613221",
   "metadata": {},
   "source": [
    "Notice how we keep the functions performing one task and how the function name is `verb_noun`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b6338-fd2c-4e5b-be75-39f7d8a0d510",
   "metadata": {},
   "source": [
    "**Exercise**: How would you improve the `check_variance` method?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81df2bf-61b5-4bd6-9017-bc021b03660a",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd37db0-7496-4dc0-b03e-7994af55564d",
   "metadata": {},
   "source": [
    "## Code testing\n",
    "\n",
    "We will use `pytest` to test our code. Let's write a test case to test the `create_dataset` function for the `dim_customer` dataset. The test code is at **[de_project/tests/unit/dim_customer.py](https://github.com/josephmachado/de_project/blob/main/de_project/tests/unit/test_dim_customer.py)**.\n",
    "\n",
    "### **Recommended reading**: **[How to use pytest to test your code](https://www.startdataengineering.com/post/code-patterns/#4-testing-with-pytest)**\n",
    "\n",
    "We can run the tests via the terminal using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52e33dae-f49b-4e4b-be52-d4e56319bb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.4, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: /home/josephkevinmachado/code/de_project\n",
      "plugins: anyio-4.4.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "de_project/tests/unit/test_dim_customer.py \u001b[32m.\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python -m pytest de_project/tests/unit/test_dim_customer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2321849-0a2d-463d-9bf7-59a68dec8d0e",
   "metadata": {},
   "source": [
    "We will add the below code to our unit test case and run pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3beae4-cdd5-4ccd-b12d-5cdb323ced1c",
   "metadata": {},
   "source": [
    "## Orchestration and scheduling\n",
    "\n",
    "We will run `Apache Airflow` to schedule and orchestrate our pipeline. We will set the schedule to be every 1 min (so that we can check them quickly) and we only have to run **[this function](https://github.com/josephmachado/de_project/blob/b9287d8e3a78f91626da71e8ed886875095f59dc/de_project/run_pipeline.py#L7)** which runs the ETL and outputs the `customer_outreach_metrics` data.\n",
    "\n",
    "### **Recommended reading**: **[Why use Airflow](https://www.startdataengineering.com/post/why-to-use-orchestrators/)** & **[Docker for data engineers](https://www.startdataengineering.com/post/docker-for-de/)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c38b92a3-801e-4505-967c-f97c418100b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up tpch and metadata db files\n",
      "Creating TPCH input data\n",
      "Creating metadata table\n"
     ]
    }
   ],
   "source": [
    "! python ./setup/create_input_data.py\n",
    "# recreate data to ensure data from above sections are cleaned out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778ebe7-f7f8-47d7-b086-8955b3c9b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this your terminal, if the docker compose up command fails for you\n",
    "#! sudo mkdir -p logs plugins temp dags tests migrations data visualization de_project && sudo chmod -R u=rwx,g=rwx,o=rwx logs plugins temp dags tests migrations data visualization de_project tpch.db metadata.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de159e4-95e6-44bd-8957-e231cf713ab0",
   "metadata": {},
   "source": [
    "When running the command above on VSCode, use its terminal.\n",
    "\n",
    "Here's how you can run this via terminal (when using Jupyter notebook):\n",
    "\n",
    "<video src=\"./assets/videos/perms.mp4\" controls>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2814624-4b18-4532-a12d-8f6e18dff0fb",
   "metadata": {},
   "source": [
    "The below step takes a while (say 8min), the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cbe5c35-6768-4250-aded-f85ffa6475cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! docker compose up airflow-init && docker compose up --build -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "548760c3-e6a6-41fe-b3c6-22e9c48d25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sleep 30\n",
    "# this is to allow all airflow containers to fully start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bc3bed9-5b46-4452-9210-955198e15647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                          COMMAND                  CREATED          STATUS                    PORTS                                       NAMES\n",
      "2765ed630066   de_project-airflow-scheduler   \"/usr/bin/dumb-init …\"   2 minutes ago    Up 2 minutes (healthy)    8080/tcp                                    scheduler\n",
      "408090f4413e   de_project-airflow-webserver   \"/usr/bin/dumb-init …\"   2 minutes ago    Up 2 minutes (healthy)    0.0.0.0:8080->8080/tcp, :::8080->8080/tcp   webserver\n",
      "7f022043adf0   postgres:16                    \"docker-entrypoint.s…\"   11 minutes ago   Up 11 minutes (healthy)   0.0.0.0:5432->5432/tcp, :::5432->5432/tcp   postgres\n"
     ]
    }
   ],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba8a90-32cb-44f6-8ef9-3f639a6b3328",
   "metadata": {},
   "source": [
    "Go to [http://localhost:8080](http://localhost:8080) or if you are running on GitHub Codespaces, go to the `ports` tab and click on the link for port 8080(like shown below).\n",
    "\n",
    "**username**: airflow\n",
    "\n",
    "**password**: airflow\n",
    "\n",
    "![Open Airflow on GitHub Codespaces](./assets/images/cs3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c11338-784b-40ff-a812-8203aa4eb1c5",
   "metadata": {},
   "source": [
    "Your DAG will look like the below\n",
    "\n",
    "![TPCH ETL DAG](./assets/images/dagdep.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00b511bf-7880-44e1-8ad8-dcccdbec92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! docker compose down --volumes --rmi all\n",
    "# shut down docker containers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237382c-e853-4f32-9de1-ce40e11b916a",
   "metadata": {},
   "source": [
    "# What data project to build\n",
    "\n",
    "Check out this post, **[that goes over how to decide on a data project for your portfolio](https://www.startdataengineering.com/post/what-data-project-to-build/)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e06d9-0000-43b7-b667-fef0f29069d0",
   "metadata": {},
   "source": [
    "# Note: If you liked this you'll enjoy my 3 weekend intensive data engineering hands-on workshop, **[sign up here](https://astounding-architect-5764.ck.page/684e1f422f)**\n",
    "\n",
    "# Please provide feedback(anonymously) at https://form.typeform.com/to/AyUYk4RZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a8f27-1f80-4665-94e6-4c67bb3eb9a5",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "In the next post, we will cover the following:\n",
    "\n",
    "1. Setting up a proper DQ system\n",
    "2. Creating a visualization dashboard\n",
    "3. Moving common parts of code to the base class\n",
    "4. Persisting tables in a cloud storage system (e.g. S3)\n",
    "5. Deploying the pipeline to the cloud with Terraform\n",
    "6. Setting up monitoring and alerting infrastructure\n",
    "7. Setting up a dashboard to visualize the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0111003-0ab1-4c8c-ab22-c45a08a77d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
